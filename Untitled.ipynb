{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7883b130-8a65-4ec8-9d07-93fd30ba1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\bronto1\\dev\\virtenvs\\genai_groundup\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(linewidth=300)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Load data and preprocess\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train = X_train / np.max(X_train)\n",
    "X_test = X_test / np.max(X_test)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "epochs = 5000\n",
    "learning_rate = 0.002\n",
    "hidden_size = 10\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Setting a random seed ensures that the random number generation is reproducible, \n",
    "    # which means that you'll get the same set of 'random' numbers (and therefore the same \n",
    "    # initial weights and biases) every time you run your code. \n",
    "np.random.seed(168)\n",
    "    \n",
    "\n",
    "def initialize_network(input_size, hidden_size, output_size):\n",
    "    weights = {\n",
    "        \"W1\": np.random.randn(input_size, hidden_size) * 0.01,\n",
    "        \"W2\": np.random.randn(hidden_size, output_size) * 0.01\n",
    "    }\n",
    "    biases = {\n",
    "        \"b1\": np.zeros((1, hidden_size)),\n",
    "        \"b2\": np.zeros((1, output_size))\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "def forward(X, weights, biases):\n",
    "    Z1 = np.dot(X, weights[\"W1\"]) + biases[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, weights[\"W2\"]) + biases[\"b2\"]\n",
    "    A2 = softmax(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0) * 1\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def backward(X, y, cache, weights, biases):\n",
    "    error_output = cache[\"A2\"] - y\n",
    "    d_W2 = np.dot(cache[\"A1\"].T, error_output)\n",
    "    d_b2 = np.sum(error_output, axis=0, keepdims=True)\n",
    "\n",
    "    error_hidden = np.dot(error_output, weights[\"W2\"].T) * relu_derivative(cache[\"Z1\"])\n",
    "    d_W1 = np.dot(X.T, error_hidden)\n",
    "    d_b1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    grads = {\"d_W1\": d_W1, \"d_b1\": d_b1, \"d_W2\": d_W2, \"d_b2\": d_b2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(weights, biases, grads, learning_rate):\n",
    "    weights[\"W1\"] -= learning_rate * grads[\"d_W1\"]\n",
    "    biases[\"b1\"] -= learning_rate * grads[\"d_b1\"]\n",
    "    weights[\"W2\"] -= learning_rate * grads[\"d_W2\"]\n",
    "    biases[\"b2\"] -= learning_rate * grads[\"d_b2\"]\n",
    "    return weights, biases\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    epsilon = 1e-12\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "def predict(X, weights, biases):\n",
    "    y_pred, _ = forward(X, weights, biases)\n",
    "    return np.argmax(y_pred, axis=1)\n",
    "\n",
    "weights, biases = initialize_network(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8fbd45bd-2b48-4b88-9d62-14c4887b2e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0053,  0.0248, -0.0077, -0.012 ,  0.0042,  0.0097,  0.0009,  0.0174,  0.0146, -0.0057],\n",
       "       [-0.0153,  0.0237, -0.0028,  0.0139, -0.0061, -0.0123, -0.0125, -0.0017, -0.001 ,  0.0017],\n",
       "       [-0.0031,  0.0032, -0.0119,  0.0119, -0.0158, -0.0006, -0.004 ,  0.    , -0.0224, -0.0072],\n",
       "       [-0.0111,  0.018 ,  0.0144, -0.0113,  0.0068, -0.0045,  0.0064, -0.0013, -0.0017,  0.    ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['W1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7548078f-6b83-40b5-a9bb-de70ddec06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, cache = forward(X_train, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c8b01feb-0025-4d54-8852-e81fe3d2b235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331],\n",
       "       [0.3335, 0.3333, 0.3332],\n",
       "       [0.3336, 0.3333, 0.3331]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['A2']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
